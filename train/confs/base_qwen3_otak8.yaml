# Example configuration for training the graph environment with SkyRL.
# Based on src/modeling/confs/graph_grpo.yaml and SkyRL search example.

# Dataset paths (produced via src/skyrl/prepare_graph.py)
defaults:
  - ppo_base_config
  - _self_

data:
  train_data: ["train.parquet"]
  val_data: ["validation.parquet"]

trainer:
  # Switch to `deepspeed` here to use the DeepSpeed backend
  strategy: fsdp2

  algorithm:
    advantage_estimator: "gae"
    use_kl_estimator_k3: true
    use_abs_kl: false
    # note: use_kl_in_reward and use_kl_loss should be mutually exclusive
    use_kl_in_reward: false # apply kl loss to rewards
    use_kl_loss: false # used in policy model
    kl_loss_coef: 0.001
    # this adds training batch level normalization to advantages
    advantage_batch_normalize: false
    value_head_prefix: "value_head"
    ppo_loss_type: "regular" # "regular", "dual_clip"
    loss_reduction: "sequence_mean" # "token_mean", "sequence_mean"

    # GAE parameters
    lambd: 1.0
    gamma: 1.0

    # PPO parameters
    eps_clip_low: 0.2
    eps_clip_high: 0.28
    # dual clip parameters
    clip_ratio_c: 3.0

    # value loss parameters
    value_clip: 0.2
    normalize_reward: true

  sequence_parallel_backend: "ulysses"

  policy:
    model:
      path: open-thoughts/OpenThinker-Agent-v1
    optimizer_config:
      lr: 5.0e-7
      max_grad_norm: 0.1 
      num_warmup_steps: 20
      weight_decay: 0.01
      offload_after_step: true
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true
      fsdp_size: -1
    use_torch_compile: true
    sequence_parallel_size: 1
  ref:
    fsdp_config:
      cpu_offload: true
    policy:
  critic:
    model:
      path: open-thoughts/OpenThinker-Agent-v1
    optimizer_config:
      lr: 1.0e-6
      max_grad_norm: 0.1 
      num_warmup_steps: 20
      weight_decay: 0.01
      offload_after_step: true
    fsdp_config:
      cpu_offload: false
      reshard_after_forward: true
      fsdp_size: -1
    use_torch_compile: true
    sequence_parallel_size: 1


  placement:
    policy_num_gpus_per_node: 8
    ref_num_gpus_per_node: 8
    critic_num_gpus_per_node: 8

  epochs: 10
  update_epochs_per_batch: 2
  train_batch_size: 16
  policy_mini_batch_size: 16
  micro_forward_batch_size_per_gpu: 2
  micro_train_batch_size_per_gpu: 2
  critic_mini_batch_size: 8
  update_ref_every_epoch: true
  max_prompt_length: 1536
  eval_batch_size: 256
  eval_before_train: false
  eval_interval: 50
  gradient_checkpointing: true
  seed: 42

  resume_mode: null # null/"none", "latest", "from_path"
  resume_path: null
  ckpt_path:  # Path for resumable training checkpoints (model state, optimizer state, etc.)
  max_ckpts_to_keep: 15 # -1 to keep all checkpoints, N to keep the last N checkpoints
  ckpt_interval: 50  # Save full training checkpoint every `ckpt_interval` steps.
  hf_save_interval: -1  # Save HF format model(s)every `hf_save_interval` steps.
  export_path:  # Path for exported artifacts (HF models, debug dumps, etc.)
  project_name: "simrl-sky-endless"
  run_name: "endless-ppo-openthinker-agent-v1-sft-0124-endless"
  logger: "wandb"


# Inference/generation settings
generator:
  model_dtype: "bfloat16" # should match dtype for inference engine
  run_engines_locally: true
  num_inference_engines: 8
  backend: "vllm"
  weight_sync_backend: "nccl"
  inference_engine_tensor_parallel_size: 1
  n_samples_per_prompt: 16
  async_engine: true
  batched: false
  logprobs: null 
  max_input_length: 16384 # max generator input length used for multi-turn conversations - for single turn set equal to max_prompt_length
  enable_prefix_caching: true
  enable_chunked_prefill: true
  max_num_batched_tokens: 32768
  enforce_eager: false
  gpu_memory_utilization: 0.85
  max_num_seqs: 16 
  remote_inference_engine_urls: ["127.0.0.1:8001"]
  max_turns: 16

  override_existing_update_group: "auto" # "auto", "enable", "disable"
  # sampling params for generation phase
  sampling_params:
    max_generate_length: 2048
    temperature: 0.6
    top_p: 1.0
    min_p: 0.0
    top_k: -1

  use_conversation_multi_turn: true
  apply_overlong_filtering: false


  # sampling params for evaluation
  eval_sampling_params:
    max_generate_length: 2048
    temperature: 0.6
    top_p: 1.0
    min_p: 0.0
    top_k: -1
  # number of samples per prompt for evaluation
  eval_n_samples_per_prompt: 1
  zero_reward_on_non_stop: false


# Environment configuration
environment:
  env_class: endless
  skyrl_gym:
    max_env_workers: 32
    endless:
      max_turns: 16
